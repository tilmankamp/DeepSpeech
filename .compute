#!/bin/bash

# this script gets executed on every node of an allocation;
# in our case we use the provided COMPUTE_* env variables
# to construct a distributed TensorFlow cluster definition

set -o pipefail

# activating standard TensorFlow Python virtual environment
source /usr/local/tensorflow/bin/activate

# the standard script to execute
if [ -f ".run" ]; then
  base_cmd="./.run"
else
  base_cmd="bin/run-wer-automation.sh"
fi

# reading the comma separated node list into array "nodes"
IFS=',' read -r -a nodes <<< "$COMPUTE_NODES"
# keep fist node for convenience
first=${nodes[0]}
# hostname for debugging
hostname=`hostname`
# log timestamp prefix
time_format='[%Y-%m-%d %H:%M:%.S]'
sub_processes=1

# there is more than one node so we will build a cluster definition

# defining all cluster ports in a way that avoids collisions with other cluster allocations
# (that could eventually get scheduled on the same node)
((port_base=10000 + (COMPUTE_JOB_NUMBER * 100) % 50000))
((coord_port=port_base))
((ps_port=port_base + 1))
((worker_port=port_base + 2))
for node in "${nodes[@]}"; do
    worker_hosts[$worker_port]="$node:$worker_port"
    ((worker_port=worker_port + 1))
done

# converting worker_hosts array of host:port pairs into a comma separated list
worker_hosts=$(printf ",%s" "${worker_hosts[@]}")
worker_hosts=${worker_hosts:1}

# shared cluster configuration
# assert: for this job it should be exactly the same on all allocated nodes
cluster="--coord_host $first --nodes=$worker_hosts"

# helpful for debugging potential networking issues
echo "Starting allocated node no. $COMPUTE_NODE_INDEX on $hostname of cluster (workers: $worker_hosts) ..."

# starting the worker
logfile=../worker_$COMPUTE_NODE_INDEX.log
touch $logfile # guarantees existence for "tail -f *.log"
$base_cmd $cluster --job_name worker --task_index $COMPUTE_NODE_INDEX "$@" 2>&1 | ts "$time_format [worker $COMPUTE_NODE_INDEX]" >$logfile
